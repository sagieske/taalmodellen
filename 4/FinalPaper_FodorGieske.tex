\documentclass[10pt, a4paper]{article}

\usepackage{fullpage}
\usepackage{color}
\usepackage{amsmath}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{algorithm2e}
\usepackage{mathtools}


% Note by Eszter: Ik lul maar wat...fucking taalmodellen
% Note 2: Het structuur van de paper is nog niet echt goed, maar dat komt tijdens het schrijven

% On another note: 
%   - uitleg van calculateNgram?
%   - uitleg van parseWallstreet?

\title{Final Paper: Language models BSc AI 2013 \\ Assignment 4}
\author{Eszter Fodor, Sharon Gieske \\ (5873320, 6167667)}
\date{\today}

\begin{document}
\maketitle{}

\abstract{This paper discribes our implementation of a probabilistic Markov POS tagger, trained and tested on
various sections of the Wall Street Journal. The POS tagger is trained by creating a language and task model, trained with smoothing and without smoothing. The test corpus is tagged using the Viterbi algorithm. A POS tagger trained with smoothed models shows to have a good accuracy and recall. }

\section*{Introduction}
The problem that needed to be solved during the four assignments was how to calculate the most
probable Part-Of-Speach (POS) tag sequence using tagging and smoothing on a training corpus. The
program that needed to be written would eventually be able to tag the test corpus with POS tags.

Some POS-tag sequences are less possible than others, this information can be used to analyze 
texts to learn whether it is grammatically correct or not.
Another importance of POS-tagging is to disambiguate words within a sentence. This can be done by
taking the previous words and their POS-tags into account. A program that is able to tag sentences
accurately can be used on various corpora to learn what kind of sentences it consists of. 

\section*{Approach}
This section describes the steps that were taken to construct a working program that is able to POS-tag a corpus as accurately as possible.


\subsection*{Basics}
There are various models which form the base of a working POS-tagger. These models need to be implemented in order to successfully build a tagger. The implementation of these models was the objective of the first three assignments of the course Natural Language Models. A small portion of the functions within the implemented programs were not used to construct the tagger and will therefore not be discussed in this paper.

The remaining implemented functions, such as constructing \textit{ngrams}, implementation of \textit{Good-Turing smoothing}, implementation of the \textit{Viterbi algorithm} will be discussed further along.

\subsubsection*{N-grams}
N-grams are sequences of n items from a sequence of text. N-grams are constructed using tuples of n items. A dictionary is created using these n-grams to obtain their frequency and is used for calculating sequence probabilities.

\subsubsection*{Good-Turing smoothing}
Good-Turing smoothing is a technique to predict the probability of occurances of items not seen in training. Good-Turing uses the count of things seen once to estimate the count of unseen items using the following equation:
\begin{equation}
r_0 = \frac{n_1}{n_0}, \text{ for }r = 0\\
\end{equation}
Combining Good-Turing smoothing with Katz-Backoff allows discounting over events occuring $1 \leq r \leq k$ times.
\begin{equation}
r^* = \frac{(r+1)\frac{n_{r+1}}{n_r}-r\frac{(k+1)n_{k+1}}{n_1}}{1-\frac{(k+1)n_{k+1}}{n_1}}, \text{ for } 1 \leq r \leq k
\end{equation}

\subsection*{Training}
Sections 2 to 21 from the Wall Street Journal formed the training corpus for this project. Every word in this corpus has been previously POS-tagged, taking the context within the sentence into account. These words, along with their tags, were loaded as tuples in the form of $(word, tag)$ into the program to be used later on. Besides the tuples for word-tag pairs, a dictionary with unigrams of every word in the corpus along with their frequencies is created. Furthermore, trigrams are created from all the POS-tag sequences within the corpus along with their frequencies, which forms a so called \textit{language model}. 

Another model that needed to be implemented is the so called \textit{lexical model}, which is named \textit{task model} in the code to prevent confusion while programming. This model depends on the the words and the tags that occur in the corpus and on the dictionary with POS-tag unigrams mentioned above. Using these three acquired data the lexical model is created by calculating the probability of the word-tag pair. This probability is gained by counting the occurances of the tag given by that word divided by the total occurances of this tag. Furthermore, the possibilities found for POS-tags for a word are being kept as well as the total occurances of POS-tags.

When running the program smoothing is by default enabled which means that the two models are smoothed, though both in different ways. The \textit{language model} is smoothed using Good-Turing smoothing with Kats Backoff with $k = 4$. In the case of the lexical model, only events with frequencies of $0$ and $1$ are smoothed using the formulas:\\

\centerline{$0^* = \frac{1}{2} \cdot \frac{n_1 (t)}{n_0}$ and $1^* = \frac{1}{2}$}\\\

The tagger is trained, using all of the above, by calculating the probabilities for the POS-tag sequences. Unfortunatly the training takes a relatively long time because the calculations for the lexical model take much data into account and it takes time to process it all, especially with a corpus that is as big as the sections of the Wall Street Journal.

\subsection*{Tagging}
The next step, after the training of the tagger is tagging of the test corpus. This corpus consists of the sentences from section 23 of the Wall Street Journal. The tagging of this corpus is done by using the \textit{Viterbi algorithm}. This algorithm uses \textit{emission probabilities} and \textit{transition probabilities} to calculate the most probable tag sequencies. 

Emission probabilty the probability of a word, given the current state (the POS-tag in this case). This probability is calculated by applying the lexical model to every word and their tags. Transition probability is the probability of a state given the previous states and in this case the probability of a POS-tag, given the previous two POS-tags within the sentence.

After calculating these probabilities, the most probable route between states is calculated by recursively calculating the most probable transition between states. How the algorithm works is demonstrated in the appendix by \textit{Algorithm 2} and \textit{Algorithm 3}.

The words from the test corpus, along with their POS-tags that were calculated by the steps described above, are written in an output file. To check whether the tagged did a good job, the output file needs to be compared with the provided test corpus and the tags that were manually added to it. Precision and recall are calculated to determine the quality of the POS tagger.


\subsection*{Testing}
The program can be run by:
\begin{verbatim}
$ python taalmodellen4.py training.pos test.pos output.pos
\end{verbatim}

With this command the program is fully run including smoothing, tagging and comparing the resulting tags to the tags in test corpus. The output that is created is an output file with the words from the test corpus and their tags and in the console the \textit{precision} and the \textit{recall} of the tagger is printed along with the time that it took the program to finish. The results of the program can be found in section \textit{Results}.

\section*{Results}

\noindent The results of the final program with smoothing enabled:
\begin{verbatim}
sharon@sharon-Aspire-5749 ~/Documents/uva/taalmodellen/taalmodellen/4 $ python 
taalmodellen4.py training.pos test.pos output.pos
> Smoothing enabled
> Write output to bla.pos
** Loading train corpus
** Loading test corpus
** Calculating unigram
** Calculating language model
** Calculating task model
*** Processing 42916 sentences
** Smooth language model
** Smooth task model
** Calculate special bigram
** Start Tagging
-----------------------
Total words: 47066
Total words tagged: 45821
Precision: 80.244866%
Recall: 78.122211%
Time taken: 55 min and 22 sec
\end{verbatim}

\noindent The results of the final program with smoothing disabled:
\begin{verbatim}
eszter@eszter-laptop /media/DATA/AI/taalmodellen/4 $ python taalmodellen4.py 
training.pos test.pos nonSmoothTest.pos --no-smoothing
> Smoothing disabled
> Write output to nonSmoothTest.pos
** Loading train corpus
** Loading test corpus
** Calculating unigram
** Calculating language model
** Calculating task model
*** Processing 42916 sentences
** Start Tagging
-----------------------
Total words: 47066
Total words tagged: 45821
Precision: 55.956876%
Recall: 54.476692%
Time taken: 55 min and 22 sec
\end{verbatim}

\section*{Conclusion/Discussion}
There is a big difference between the precision and recall of the tagger when it is run with and without smoothing. As it can be read above, the precision of the tagger \textit{with} smoothing is 80\% and \textit{without} smoothing it is only 56\%. This enormous difference can be explained by the fact that POS-tags can occure with a very low frequency which makes it harder to calculate the highest probability for a sequence.

Though this tagger is not perfectly accurate, it does a decent job. It could deffinetly use some tweaking if one wanted to use it for research purposes, but as of this project a precision and recall of around 80\% is satisfactory.

The POS tagger with smoothing gives a very good result on the test corpus, but this does not mean the POS tagger is directly suitable for texts from other domains. The POS tagger is wholely trained on text obtained from the Wall Street Journal. Word-tag pairs are trained domain specific and it is not certain the POS tagger will perform as well on texts from other domains.

\newpage
\appendix{}
\section{Algorithms}


\begin{algorithm}[H]
\caption{Viterbi algorithm}
\textbf{Input:} sentence: sentence to be tagged\\
\textbf{Input:} langugageModel: second-order Markov model of POS-tags\\
\textbf{Input:} bigran: first-order Markov model of POS-tags\\
\textbf{Input:} taskModel: lexical model\\
\textbf{Input:} wordTags: mapping from word $\rightarrow$ tags\\
V $\leftarrow \emptyset$\\
\ForAll{word $\in$ sentence}{emission $\leftarrow \emptyset$\\
\ForAll{tag $\in$ wordTags}{emission(tag) $\rightarrow$ taskModel(word, tag)}
V $\leftarrow$ V $\cup$ emission}
transition $\leftarrow \emptyset$\\
\For{i $= 1 \rightarrow \|V\|$}{
\ForAll{e $\in V_i$}{
\ForAll{f $\in V_{i-1}$}{
\ForAll{g $\in V_{i+1}$}{
\eIf{$\langle e, f, g \rangle \in langugageModel \land \langle e, f \rangle \in bigram$}{
transition(f, e) $\rightarrow \frac{languageModel(e, f, g)}{bigram(\langle e, f \rangle)}$}
{$transitions(f, e) \rightarrow 0$ }}}}}
\Return{calculateOptimalRoute(V, transition, $\|sentence\|$)}
\end{algorithm}


\begin{algorithm}[H]
\caption{calculateOptimalRoute}
\textbf{Input:} V: a state trellis of a HMM\\
\textbf{Input:} transition: transition probabilities of a HMM\\
\textbf{Input:} i: current position in trellis\\
\eIf{$location == -1$}{\Return(1, START)}{
$\langle prob_prev, path_prev \rangle$ $\leftarrow calculateOptimalRoute(V, transitions, i-1$\\
$\langle prob, tag \rangle$ $\leftarrow argmax(prob = transition(path_prev(i-1), tag | tag \in V_i$)\\
\Return{$\langle prob_prev \cdot path \cup tag \rangle$}}
\end{algorithm}


\end{document}
